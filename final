#!/usr/bin/env python3
import os
import sys
import numpy as np
import rasterio
from rasterio.transform import from_bounds
from pathlib import Path
import psycopg2
from psycopg2.extras import RealDictCursor
import torch
import torch.nn as nn
from config import *

# ---------------------------------------------------
# OUTPUT PATH CONFIGURATION
# ---------------------------------------------------
OUTPUT_BASE_PATH = Path("/mnt/68_data/PROCESSED_DATA_THEMES/T6/T6S1/s2_ai_building_probability_T6S1P15")

# ---------------------------------------------------
# CNN MODEL
# ---------------------------------------------------
class ImprovedCNN(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        self.conv1 = nn.Conv2d(4,16,3,padding='same'); self.bn1=nn.BatchNorm2d(16)
        self.conv2 = nn.Conv2d(16,32,3,padding='same'); self.bn2=nn.BatchNorm2d(32)
        self.conv3 = nn.Conv2d(32,64,3,padding='same'); self.bn3=nn.BatchNorm2d(64)
        self.conv4 = nn.Conv2d(64,64,5,padding='same'); self.bn4=nn.BatchNorm2d(64)
        self.conv5 = nn.Conv2d(64,16,1); self.bn5=nn.BatchNorm2d(16)
        self.conv6 = nn.Conv2d(16,1,1)
        self.relu = nn.ReLU(); self.dropout=nn.Dropout2d(dropout_rate); self.sigmoid=nn.Sigmoid()

    def forward(self,x):
        x = self.relu(self.bn1(self.conv1(x))); x=self.dropout(x)
        x = self.relu(self.bn2(self.conv2(x))); x=self.dropout(x)
        x = self.relu(self.bn3(self.conv3(x))); x=self.dropout(x)
        x = self.relu(self.bn4(self.conv4(x)))
        x = self.relu(self.bn5(self.conv5(x)))
        x = self.sigmoid(self.conv6(x))
        return x

# ---------------------------------------------------
# PATH TRANSFORMATION
# ---------------------------------------------------
def transform_db_path_to_actual_path(db_path):
    """
    Transform database path to actual file system path
    
    /mnt/ridam/68_data/T0/T0S1/... → /mnt/68_data/PROCESSED_DATA_THEMES/T0/T0S1/...
    /mnt/ridam/115_data/T0/T0S1/... → /home/sac/115_data/T0/T0S1/...
    """
    db_path_str = str(db_path)
    
    if '/mnt/ridam/68_data/' in db_path_str:
        actual_path = db_path_str.replace('/mnt/ridam/68_data/', '/mnt/68_data/PROCESSED_DATA_THEMES/')
        return actual_path
    elif '/mnt/ridam/115_data/' in db_path_str:
        actual_path = db_path_str.replace('/mnt/ridam/115_data/', '/home/sac/115_data/')
        return actual_path
    else:
        # Return original path if no match
        return db_path_str

# ---------------------------------------------------
# DATABASE FETCH - Get file paths from published_rasters
# ---------------------------------------------------
def fetch_file_records():
    """
    Fetch source file paths from database for T0S1P0 dataset
    within specified spatial extent
    """
    query = """
    SELECT
        data_id,
        source_file_path,
        dataset_id,
        "timestamp"
    FROM public.published_rasters
    WHERE dataset_id = 'T0S1P0'
      AND "timestamp" >= TIMESTAMPTZ '2025-01-01 00:00:00+05:30'
      AND ST_Intersects(
            bounds,
            ST_MakeEnvelope(
                68.1, 20.1,
                74.5, 24.7,
                4326
            )
          )
    ORDER BY "timestamp"
    """
    
    try:
        conn = psycopg2.connect(
            host=DB_HOST, 
            port=DB_PORT, 
            dbname=DB_NAME, 
            user=DB_USER, 
            password=DB_PASSWORD
        )
        
        records = []
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query)
            records = cur.fetchall()
        
        conn.close()
        return records
    
    except Exception as e:
        print(f"✗ Database error: {e}")
        return []

# ---------------------------------------------------
# UTILITY FUNCTIONS
# ---------------------------------------------------
def save_raster_tif(filename, array, bbox, projection='EPSG:4326'):
    """Save numpy array as GeoTIFF with proper metadata"""
    if array.ndim == 2:
        height, width = array.shape
        array = array.reshape(1, height, width)
    else:
        _, height, width = array.shape

    transform = from_bounds(*bbox, width, height)
    
    with rasterio.open(
        filename, 'w', 
        driver='GTiff',
        height=height, 
        width=width, 
        count=array.shape[0],
        dtype=array.dtype, 
        crs=projection, 
        transform=transform,
        compress='lzw'
    ) as dst:
        for i in range(array.shape[0]):
            dst.write(array[i], i+1)

def standardize_channels(msi_array, target_channels=4):
    """Standardize MSI array to target number of channels"""
    c, h, w = msi_array.shape
    if c == target_channels:
        return msi_array
    elif c > target_channels:
        return msi_array[:target_channels]
    else:
        padded = np.zeros((target_channels, h, w), dtype=msi_array.dtype)
        padded[:c] = msi_array
        return padded

def apply_cloud_mask(msi_array, cloud_band_values=[4,5,7], fill_value=-1):
    """Apply cloud masking based on SCL band values"""
    last_band = msi_array[-1]
    mask = np.isin(last_band, cloud_band_values)
    msi_masked = msi_array.copy()
    for i in range(msi_array.shape[0]):
        msi_masked[i, ~mask] = fill_value
    return msi_masked, mask

def is_cloudy_tile(scl_band, threshold=0.80):
    """
    Check if tile exceeds cloud threshold
    Cloud classes: 8=medium clouds, 9=high clouds, 10=thin cirrus, 11=snow/ice
    """
    cloud_classes = {8, 9, 10, 11}
    cloud_fraction = np.isin(scl_band, list(cloud_classes)).sum() / scl_band.size
    return cloud_fraction >= threshold

# ---------------------------------------------------
# PROCESS SINGLE FILE WITH CNN INFERENCE
# ---------------------------------------------------
def process_file_record(model, record, processed_set, device='cuda', threshold=0.4):
    """
    Process a single Sentinel-2 file from database record
    Apply CNN model for building probability prediction
    """
    db_path = record['source_file_path']
    timestamp = record['timestamp']
    data_id = record['data_id']
    
    # Transform database path to actual file system path
    actual_path = transform_db_path_to_actual_path(db_path)
    
    print(f"  DB Path:     {db_path}")
    print(f"  Actual Path: {actual_path}")
    
    # Check if source file exists
    if not os.path.exists(actual_path):
        print(f"  ✗ Source file not found at actual path")
        return False
    
    # Extract date components for folder structure
    year = timestamp.strftime("%Y")
    month = timestamp.strftime("%m")
    day = timestamp.strftime("%d")
    date_str = f"{year}{month}{day}"
    
    # Create output directory structure: /mnt/68_data/.../YYYY/MM/DD/
    target_dir = OUTPUT_BASE_PATH / year / month / day
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate output filename with data_id for uniqueness
    dst_file = target_dir / f"MSI_probability_{date_str}_{data_id}.tif"
    
    # Skip if already processed
    if str(dst_file) in processed_set or dst_file.exists():
        print(f"  ⊘ Already processed: {dst_file.name}")
        return False
    
    try:
        # Read source raster using rasterio
        with rasterio.open(actual_path) as src:
            arr = src.read()  # Read all bands
            bbox = src.bounds
            crs = src.crs
            
            print(f"  └─ Bands: {arr.shape[0]}, Size: {arr.shape[1]}x{arr.shape[2]}")
        
        # Check for empty tile
        if not np.any(arr != 0):
            print(f"  ✗ Empty tile (all zeros), skipping")
            return False
        
        # Check cloud coverage using SCL band (last band or band index 4)
        scl_band = arr[4] if arr.shape[0] > 4 else arr[-1]
        
        if is_cloudy_tile(scl_band, 0.8):
            cloud_pct = (np.isin(scl_band, [8,9,10,11]).sum() / scl_band.size) * 100
            print(f"  ✗ Cloudy tile ({cloud_pct:.1f}% clouds), skipping")
            return False
        
        # Standardize to 4 channels (typically B,G,R,NIR)
        arr = standardize_channels(arr, 4)
        
        # Apply cloud mask if needed
        if arr.shape[0] > 4:
            arr, _ = apply_cloud_mask(arr, [4,5,7], -1)
            arr[arr == -1] = 0
        
        # Normalize to [0, 1] range (Sentinel-2 values are typically 0-10000)
        arr_normalized = arr.astype(np.float32) / 10000.0
        
        # Run CNN inference
        model.eval()
        with torch.no_grad():
            # Convert to tensor and add batch dimension
            tensor = torch.from_numpy(arr_normalized).unsqueeze(0).to(device)
            
            # Get probability prediction
            prob = model(tensor).cpu().numpy()[0, 0]
            
            # Scale probability to 0-100 and convert to uint16
            prob_scaled = (prob * 100).astype(np.uint16)
        
        # Save probability raster
        save_raster_tif(dst_file, prob_scaled, bbox, crs)
        
        # Add to processed set
        processed_set.add(str(dst_file))
        
        # Log processed file
        with open('processed.log', 'a') as f:
            f.write(f"{dst_file}\n")
        
        print(f"  ✓ Saved: {dst_file.name}")
        return True
        
    except Exception as e:
        print(f"  ✗ ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

# ---------------------------------------------------
# MAIN PIPELINE
# ---------------------------------------------------
def run_pipeline():
    """
    Main processing pipeline:
    1. Load CNN model
    2. Fetch file records from database
    3. Transform DB paths to actual file system paths
    4. Process each file with building probability inference
    """
    
    print("="*70)
    print("SENTINEL-2 BUILDING PROBABILITY PREDICTION PIPELINE")
    print("="*70)
    print("\nPath Mapping:")
    print("  /mnt/ridam/68_data/  → /mnt/68_data/PROCESSED_DATA_THEMES/")
    print("  /mnt/ridam/115_data/ → /home/sac/115_data/")
    print()
    
    # Load processed files log to avoid reprocessing
    processed_set = set()
    if os.path.exists('processed.log'):
        with open('processed.log', 'r') as f:
            processed_set = set(line.strip() for line in f.readlines())
        print(f"Found {len(processed_set)} already processed files\n")
    
    # Load CNN model
    print("Loading CNN model...")
    model = ImprovedCNN()
    
    if not os.path.exists('best_model_seasonal.pth'):
        print("✗ Model file 'best_model_seasonal.pth' not found!")
        return
    
    model.load_state_dict(
        torch.load('best_model_seasonal.pth', map_location='cpu', weights_only=True)
    )
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    print(f"✓ Model loaded successfully")
    print(f"  Device: {device}")
    
    if device == 'cuda':
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    print(f"\nOutput directory: {OUTPUT_BASE_PATH}")
    print()
    
    # Fetch file records from database
    print("Fetching file records from database...")
    records = fetch_file_records()
    
    if not records:
        print("✗ No records found or database error")
        return
    
    print(f"✓ Found {len(records)} files to process\n")
    print("="*70)
    
    # Process each file
    success_count = 0
    skip_count = 0
    error_count = 0
    
    for i, record in enumerate(records, 1):
        print(f"\n[{i}/{len(records)}] {record['data_id']}")
        print(f"  Date: {record['timestamp'].strftime('%Y-%m-%d')}")
        
        result = process_file_record(
            model=model,
            record=record,
            processed_set=processed_set,
            device=device,
            threshold=0.4
        )
        
        if result:
            success_count += 1
        elif str(Path(OUTPUT_BASE_PATH) / record['timestamp'].strftime('%Y/%m/%d') / 
                 f"MSI_probability_{record['timestamp'].strftime('%Y%m%d')}_{record['data_id']}.tif") in processed_set:
            skip_count += 1
        else:
            error_count += 1
    
    # Summary
    print("\n" + "="*70)
    print("PROCESSING COMPLETE")
    print("="*70)
    print(f"Total files:      {len(records)}")
    print(f"✓ Processed:      {success_count}")
    print(f"⊘ Skipped:        {skip_count}")
    print(f"✗ Failed/Empty:   {error_count}")
    print("="*70)

# ---------------------------------------------------
# ENTRY POINT
# ---------------------------------------------------
if __name__ == '__main__':
    try:
        run_pipeline()
    except KeyboardInterrupt:
        print("\n\n✗ Pipeline interrupted by user")
    except Exception as e:
        print(f"\n✗ Pipeline failed with error: {e}")
        import traceback
        traceback.print_exc()
